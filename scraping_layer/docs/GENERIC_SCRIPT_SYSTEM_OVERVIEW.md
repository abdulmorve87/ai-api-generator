# Generic Script System Overview

## üéØ Purpose

This system creates a bridge between user requirements (from `components/form.py`) and the scraping layer execution engine. The AI layer will populate generic scripts that can be seamlessly executed by the scraping layer to extract meaningful data from the web.

## üèóÔ∏è Architecture

```
User Form Input ‚Üí AI Layer ‚Üí Generic Script ‚Üí Scraping Layer ‚Üí Results
     ‚Üì              ‚Üì            ‚Üì              ‚Üì             ‚Üì
  form.py    ai_script_    generic_scraping_  ScriptConfig   Data
             generator.py     script.py       (engine.py)   Output
```

## üìÅ Files Created

### 1. `generic_scraping_script.py`

**Core generic script template that AI will populate**

- **`GenericScrapingScript`**: Main data structure containing all scraping configuration
- **`FieldDefinition`**: Defines what data fields to extract and how
- **`DataSource`**: Information about target websites
- **`ResponseSchema`**: Expected output structure
- **`ScrapingRules`**: Data processing and validation rules

**Key Features:**

- Converts to `ScriptConfig` for scraping layer compatibility
- Validates completeness before execution
- Supports multiple scraping strategies (static, dynamic, hybrid)
- Includes confidence scoring and metadata

### 2. `ai_script_generator.py`

**AI layer that converts form inputs to executable scripts**

- **`AIScriptGenerator`**: Main class that processes form data
- Analyzes user requirements using NLP-like logic
- Discovers appropriate data sources
- Generates field definitions with CSS selectors
- Determines optimal scraping strategy
- Creates complete `GenericScrapingScript` instances

**Key Capabilities:**

- Keyword-based requirement analysis
- Automatic field type inference
- CSS selector generation
- Data source discovery and validation
- Confidence scoring

### 3. `script_integration_example.py`

**Complete pipeline demonstration and entry point**

- **`ScriptExecutionPipeline`**: Orchestrates the entire process
- **`execute_scraping_request()`**: Main entry point for future API layer
- Includes mock execution for testing
- Demonstrates end-to-end flow

## üîÑ Data Flow

### Step 1: User Input (form.py)

```python
form_data = {
    'data_description': 'Current IPOs with grey market premium',
    'data_source': 'chittorgarh.com',
    'desired_fields': 'company_name\nlisting_date\nissue_price',
    'response_structure': '{"data": [...]}',
    'update_frequency': 'Daily'
}
```

### Step 2: AI Processing

```python
generator = AIScriptGenerator()
script = generator.generate_script_from_form(form_data)
```

### Step 3: Script Generation

```python
GenericScrapingScript(
    script_id="ipo_scraper_abc123",
    name="Current IPOs Scraper",
    fields=[
        FieldDefinition(name="company_name", selector="td.company"),
        FieldDefinition(name="listing_date", selector="td.date"),
        # ...
    ],
    strategy=ScrapingStrategy.HYBRID,
    # ...
)
```

### Step 4: Scraping Layer Execution

```python
script_config = script.to_script_config()
result = await scraping_engine.scrape(script_config)
```

### Step 5: Formatted Output

```python
{
    "success": true,
    "data": {
        "data": [
            {"company_name": "ABC Corp", "listing_date": "2024-01-15"},
            # ...
        ]
    },
    "metadata": {
        "script_id": "ipo_scraper_abc123",
        "confidence_score": 0.91,
        # ...
    }
}
```

## üéõÔ∏è Key Components

### Field Definitions

Each field specifies:

- **Name**: Field identifier
- **Data Type**: string, number, date, url, email, etc.
- **Selector**: CSS/XPath selector for extraction
- **Validation**: Required status, patterns, transformations
- **Description**: Human-readable explanation

### Data Sources

Information about target websites:

- **URL**: Base URL for scraping
- **Reliability Score**: AI confidence in source quality
- **Anti-bot Measures**: Known protection mechanisms
- **Rate Limits**: Request frequency constraints

### Scraping Strategies

- **Static**: Simple HTML parsing
- **Dynamic**: JavaScript-rendered content
- **Hybrid**: Try dynamic, fallback to static

### Response Schema

Defines expected output structure:

- **Root Key**: Top-level data container (e.g., "data")
- **Array Format**: Whether data is array or single object
- **Field Mapping**: How extracted fields map to output
- **Sample Response**: Example of expected format

## üîß AI Layer Capabilities

### Requirement Analysis

- Keyword extraction from user descriptions
- Domain classification (finance, news, ecommerce, etc.)
- Data type inference
- Complexity assessment

### Data Source Discovery

- Known source mapping by domain
- URL validation and analysis
- Reliability scoring
- Fallback source identification

### Field Generation

- Automatic field type inference
- CSS selector generation based on field names
- Validation rule creation
- Default value assignment

### Strategy Selection

- Website analysis simulation
- Framework detection logic
- Performance optimization
- Fallback strategy planning

## üöÄ Usage Examples

### IPO Data Scraping

```python
form_data = {
    'data_description': 'IPO grey market premium data',
    'data_source': 'chittorgarh.com',
    'desired_fields': 'company_name\nissue_price\ngrey_market_premium',
    'update_frequency': 'Daily'
}

pipeline = ScriptExecutionPipeline()
result = await pipeline.execute_from_form_data(form_data)
```

### News Headlines

```python
form_data = {
    'data_description': 'Technology news headlines',
    'desired_fields': 'headline\nsummary\npublished_date',
    'response_structure': '{"articles": [...]}',
    'update_frequency': 'Hourly'
}

result = await execute_scraping_request(form_data)
```

## üîÆ Future Integration

### API Layer Integration

The `execute_scraping_request()` function serves as the main entry point:

```python
# Future API endpoint
@app.post("/api/scrape")
async def scrape_data(form_data: dict):
    return await execute_scraping_request(form_data)
```

### Real Scraping Engine

When the scraping layer is complete, replace mock execution:

```python
# Replace mock with real engine
if SCRAPING_AVAILABLE and self.scraping_engine:
    result = await self.scraping_engine.scrape(script_config)
```

### Enhanced AI Processing

Future improvements:

- Real NLP for requirement analysis
- Website content analysis for selector generation
- Machine learning for strategy optimization
- Dynamic confidence scoring

## ‚úÖ Validation & Quality

### Script Validation

- Required field checking
- Selector validation
- URL accessibility testing
- Schema compliance verification

### Confidence Scoring

- Data source reliability
- Field definition completeness
- Strategy appropriateness
- Historical success rates

### Error Handling

- Graceful degradation
- Fallback strategies
- Partial result preservation
- Detailed error reporting

## üéØ Summary

This generic script system provides:

1. **Standardized Interface**: Consistent structure for all scraping operations
2. **AI-Friendly**: Easy for AI to populate based on user requirements
3. **Scraping Layer Compatible**: Direct conversion to engine-ready format
4. **Extensible**: Easy to add new field types, strategies, and sources
5. **Validated**: Built-in validation and quality checks
6. **Production Ready**: Complete pipeline from form to results

The system is designed to be the foundation for the AI-powered API generator, providing a robust bridge between user intent and technical execution.
