# Universal Scraping Layer (Phase 1)

A simple, focused scraping system for extracting data from static HTML websites using HTTP requests and CSS selectors.

## ğŸš€ Quick Start

```python
from scraping_layer import ScrapingEngine, ScriptConfig
from scraping_layer.static_scraper import StaticScraper

# Create scraper and engine
scraper = StaticScraper()
engine = ScrapingEngine(static_scraper=scraper)

# Configure scraping
config = ScriptConfig(
    url="https://example.com",
    selectors={
        "title": "h1",
        "description": "p"
    }
)

# Execute scraping
result = await engine.scrape(config)

if result.success:
    print(f"Extracted {len(result.data)} items")
    for item in result.data:
        print(item)
```

## ğŸ“ Project Structure

```
scraping_layer/
â”œâ”€â”€ __init__.py              # Main package exports
â”œâ”€â”€ models.py                # Data models (simplified)
â”œâ”€â”€ interfaces.py            # Abstract interfaces (simplified)
â”œâ”€â”€ engine.py                # Main orchestrator (simplified)
â”œâ”€â”€ config.py                # Configuration (simplified)
â”œâ”€â”€ static_scraper.py        # Static HTML scraper (TO BE IMPLEMENTED)
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ script_execution/        # Script execution wrapper
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ executor.py
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ logging.py           # Logging utilities
```

## âœ¨ Features (Phase 1)

### Current Implementation

- âœ… **Static HTML scraping** - HTTP requests + BeautifulSoup
- âœ… **CSS selector support** - Extract specific elements
- âœ… **Simple configuration** - URL + selectors + timeout
- âœ… **Structured results** - ScrapingResult with metadata
- âœ… **Basic error handling** - Try/catch with error reporting

### Not Implemented (Future Phases)

- âŒ **Dynamic scraping** - JavaScript-rendered content (Phase 3)
- âŒ **Error retry logic** - Exponential backoff (Phase 2)
- âŒ **Data cleaning** - HTML entity decoding (Phase 2)
- âŒ **Caching** - Redis/memory-based caching (Phase 4)
- âŒ **Browser automation** - Playwright integration (Phase 3)

## ğŸ”§ Configuration

The scraping layer uses environment variables for configuration:

```bash
# Network settings
export SCRAPING_REQUEST_TIMEOUT=30
export SCRAPING_USER_AGENT="Mozilla/5.0..."

# Logging
export SCRAPING_LOG_LEVEL=INFO
```

## ğŸ“‹ Requirements

```
aiohttp>=3.9.0
beautifulsoup4>=4.12.0
lxml>=5.0.0
```

## ğŸ§ª Testing

```bash
# Run basic test (once implemented)
python -m pytest tests/test_static_scraper.py -v
```

## ğŸ—ï¸ Architecture

Simple two-layer architecture:

1. **Scraping Engine** - Orchestrates operations
2. **Static Scraper** - Fetches HTML and extracts data

## ğŸš¦ Status

**Phase 1** ğŸš§ **IN PROGRESS**

- âœ… Spec simplified
- âœ… Models simplified
- âœ… Interfaces simplified
- âœ… Engine simplified
- âœ… Config simplified
- â³ StaticScraper implementation (NEXT)

**Next Steps:**

- Implement StaticScraper class
- Add HTTP fetching with aiohttp
- Add BeautifulSoup extraction
- Write basic tests

## ğŸ“„ License

Part of the AI API Generator project.
