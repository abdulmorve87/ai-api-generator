# Universal Scraping Layer

A comprehensive scraping system that handles both static and dynamic websites with AI-generated script execution, security sandboxing, and intelligent strategy selection.

## ğŸš€ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Test the scraper (from project root)
python test_scraper.py https://example.com title=h1 description=p

# Or run directly from examples directory
cd scraping_layer/examples
python test_scraper.py https://example.com title=h1 description=p
```

## ğŸ“ Project Structure

```
scraping_layer/
â”œâ”€â”€ __init__.py              # Main package exports
â”œâ”€â”€ models.py                # Data models and types
â”œâ”€â”€ interfaces.py            # Abstract interfaces
â”œâ”€â”€ engine.py                # Main orchestrator
â”œâ”€â”€ config.py                # Configuration management
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ logging.py           # Logging utilities
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ README.md            # Examples documentation
â”‚   â”œâ”€â”€ test_scraper.py      # Interactive test script
â”‚   â””â”€â”€ debug_scraper.py     # Debug and diagnostics
â””â”€â”€ docs/
    â””â”€â”€ USAGE.md             # Detailed usage guide
```

## âœ¨ Features

### Current (Task 1 Complete)

- âœ… **Static website scraping** - HTTP requests + BeautifulSoup
- âœ… **CSS selector support** - Extract specific elements
- âœ… **Data cleaning** - HTML entity decoding, whitespace normalization
- âœ… **Error handling** - Graceful failure with detailed logging
- âœ… **Performance metrics** - Timing and extraction statistics
- âœ… **Configuration system** - Environment-based configuration
- âœ… **Structured logging** - JSON logs with context
- âœ… **Testing framework** - Property-based testing with Hypothesis

### Upcoming (Tasks 2-15)

- ğŸš§ **Dynamic website scraping** - Playwright browser automation
- ğŸš§ **AI script execution** - Sandboxed execution environment
- ğŸš§ **Content detection** - Framework identification
- ğŸš§ **Anti-bot handling** - User agent rotation, delays
- ğŸš§ **Caching system** - Redis/memory-based caching
- ğŸš§ **Browser management** - Instance pooling and cleanup
- ğŸš§ **Template system** - BeautifulSoup/Playwright templates

## ğŸ§ª Testing

```bash
# Run basic tests
cd scraping_layer/examples
python test_scraper.py https://example.com

# Run debug tests
python debug_scraper.py

# Run unit tests
cd ../..
python -m pytest tests/ -v
```

## ğŸ“– Documentation

- **[Usage Guide](docs/USAGE.md)** - Detailed usage instructions
- **[Examples](examples/README.md)** - Example scripts and patterns
- **[Requirements](../docs/kiro-spec.md)** - Original project specification
- **[Design](../.kiro/specs/universal-scraping-layer/design.md)** - Architecture and design
- **[Tasks](../.kiro/specs/universal-scraping-layer/tasks.md)** - Implementation roadmap

## ğŸ”§ Configuration

The scraping layer uses environment variables for configuration:

```bash
# Security settings
export SCRAPING_MAX_EXECUTION_TIME=300
export SCRAPING_MAX_MEMORY_MB=512

# Browser settings
export SCRAPING_MAX_BROWSERS=5
export SCRAPING_HEADLESS=true

# Cache settings
export SCRAPING_CACHE_BACKEND=memory
export SCRAPING_CACHE_TTL=3600

# Logging
export SCRAPING_LOG_LEVEL=INFO
```

## ğŸ—ï¸ Architecture

The system follows a layered architecture:

1. **API Layer** - ScrapingEngine (main interface)
2. **Detection Layer** - ContentDetector (website analysis)
3. **Execution Layer** - StaticScraper, DynamicScraper
4. **Support Services** - BrowserManager, CacheManager, ErrorHandler
5. **Data Layer** - DataExtractor, validation, cleaning

## ğŸ¤ Integration

The scraping layer integrates with the main AI API Generator:

```python
from scraping_layer import ScrapingEngine, ScriptConfig, ScrapingStrategy

# Create configuration
config = ScriptConfig(
    url="https://example.com",
    script_type=ScrapingStrategy.STATIC,
    selectors={"title": "h1", "content": "p"}
)

# Execute scraping
engine = ScrapingEngine(...)  # Inject dependencies
result = await engine.scrape(config)

# Use extracted data
if result.success:
    data = result.data
    # Serve via API endpoints
```

## ğŸ“‹ Requirements

All dependencies are listed in the main project `requirements.txt` file.

**Core dependencies:**

- `requests` - HTTP client
- `beautifulsoup4` - HTML parsing
- `playwright` - Browser automation
- `aiohttp` - Async HTTP
- `pytest` - Testing framework

Install all dependencies from the project root:

```bash
pip install -r requirements.txt
```

## ğŸš¦ Status

**Task 1: Project Setup** âœ… **COMPLETE**

- Core interfaces and models
- Configuration system
- Logging framework
- Basic static scraping
- Testing infrastructure

**Next: Task 2** - Content Detector implementation

## ğŸ“„ License

Part of the AI API Generator project.
